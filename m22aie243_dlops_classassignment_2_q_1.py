# -*- coding: utf-8 -*-
"""M22AIE243_DLOps_ClassAssignment_2_Q_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14jiyYteSRDnl3LCSHTRtlWo7kPdfwnsQ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from torch.utils.tensorboard import SummaryWriter

# Define transformations for the dataset
transform = transforms.Compose([
    transforms.Grayscale(),  # Convert to grayscale
    transforms.ToTensor(),   # Convert to tensor
    transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values to range [-1, 1]
])

# Load USPS dataset
train_dataset = datasets.USPS(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.USPS(root='./data', train=False, download=True, transform=transform)

# Define batch size
batch_size = 64

# Create data loaders
# Create data loaders with proper batch size and shuffling
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define MLP model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(16*16, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)

    def forward(self, x):
        x = x.view(-1, 16*16)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Define CNN model
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.fc1 = nn.Linear(64*4*4, 128)  # Adjusted linear layer input size
        self.fc2 = nn.Linear(128, 10)      # Add the missing fc2 layer

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = x.view(-1, 64*4*4)  # Reshape to match the correct size
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize models
mlp_model = MLP().to(device)
cnn_model = CNN().to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)
cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)

# Initialize TensorBoard writers
mlp_writer = SummaryWriter('logs/mlp')
cnn_writer = SummaryWriter('logs/cnn')

# Train MLP model
def train_mlp(model, criterion, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            optimizer.zero_grad()
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            if i % 10 == 9:  # Log every 10 mini-batches
                mlp_writer.add_scalar('loss', loss.item(), epoch * len(train_loader) + i)
        print(f"MLP Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}")
    mlp_writer.close()

# Train CNN model
def train_cnn(model, criterion, optimizer, train_loader, epochs=10):
    model.train()
    try:
        for epoch in range(epochs):
            running_loss = 0.0
            for i, (inputs, labels) in enumerate(train_loader):
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(inputs)
                # Squeeze the labels tensor to remove the extra dimension
                labels = labels.squeeze()
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
                if i % 10 == 9:  # Log every 10 mini-batches
                    cnn_writer.add_scalar('loss', loss.item(), epoch * len(train_loader) + i)
            print(f"CNN Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}")
    except ValueError as e:
        print(f"Error occurred: {e}")
    finally:
        cnn_writer.close()


# Train MLP and CNN models
train_mlp(mlp_model, criterion, mlp_optimizer)
train_cnn(cnn_model, criterion, cnn_optimizer, train_loader)

# Evaluate MLP and CNN models
def evaluate(model, loader, name='MLP'):
    model.eval()
    correct = 0
    total = 0
    total_loss = 0
    predicted_labels = []
    true_labels = []
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            predicted_labels.extend(predicted.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    accuracy = 100 * correct / total
    avg_loss = total_loss / len(loader)
    precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')
    conf_matrix = confusion_matrix(true_labels, predicted_labels)
    print(f"{name} Evaluation:")
    print(f"Accuracy: {accuracy:.2f}%, Average Loss: {avg_loss:.4f}")
    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1_score:.4f}")
    print(f"Confusion Matrix:\n{conf_matrix}")

# Evaluate MLP and CNN models
try:
    evaluate(mlp_model, test_loader, name='MLP')
    evaluate(cnn_model, test_loader, name='CNN')
except ValueError as e:
    print(f"Error occurred during evaluation: {e}")

!pip install tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=./logs